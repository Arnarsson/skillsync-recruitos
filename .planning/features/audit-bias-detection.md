# Audit: Bias Detection and Anti-Gaming Systems

**Auditor:** audit-researcher
**Date:** 2026-02-16
**Context:** Pre-feature planning for recruiter psychological safety features
**Status:** ‚úÖ Complete

---

## Executive Summary

RecruitOS has **strong anti-gaming filters** and **comprehensive EU AI Act compliance** infrastructure, but **lacks bias detection mechanisms**. The system can detect low-quality GitHub profiles but cannot warn recruiters about potential demographic bias, protected class inference, or AI hallucinations in psychometric assessments.

**Critical Gap:** Gartner predicts 25% of applications will be fake by 2028. The system needs both anti-gaming AND bias detection to build recruiter trust.

---

## 1. Anti-Gaming Filters ‚úÖ STRONG

### Location
- **Implementation:** `lib/anti-gaming-filters.ts`
- **Tests:** `tests/anti-gaming-filters.test.ts` (17 tests, 3 pre-existing failures)
- **Status:** Comprehensive, production-ready

### Capabilities

#### 1.1 Tutorial Repository Detection
```typescript
isTutorialRepository(repo) ‚Üí boolean
```
- **Pattern matching:** 25 regex patterns (tutorial, learning, course, bootcamp, kata, etc.)
- **Boilerplate detection:** 11 exact-match project names (hello-world, test-repo, my-first-repo)
- **Topic filtering:** Flags repos with tutorial/learning/practice topics
- **Size heuristics:** Small repos (<100 KB) with no stars flagged if name contains "test/temp/tmp"

**Verdict:** Excellent coverage of learning projects.

#### 1.2 Commit Burst Detection
```typescript
detectCommitBursts(events) ‚Üí { hasBursts, burstDays, details }
```
- **Anomaly detection:** Flags days with >5x average commits AND >20 commits
- **Pattern analysis:** Requires ‚â•2 suspicious burst days OR >100 commits in single day
- **Use case:** Detects artificial activity inflation (e.g., bulk-importing repos)

**Verdict:** Good heuristic, but can't detect sophisticated gaming (e.g., scheduled commits spread over weeks).

#### 1.3 Repository Quality Analysis
```typescript
analyzeRepositoryQuality(repo, octokit, username) ‚Üí RepositoryQuality
```
**Scoring factors:**
- Fork vs. original repo (-20 points if fork without substantive commits)
- Tutorial detection (-40 points)
- Substantive commits (+30 points, excludes "initial commit", "update readme", "fix typo")
- Maintenance level: active (<30 days), maintained (<180 days), stale
- Stars: >50 stars (+15), >10 (+10), >5 (+5)

**Verdict:** Robust scoring, but "substantive commits" filter is simplistic (commit message heuristic).

#### 1.4 Comprehensive Quality Signals
```typescript
calculateQualitySignals(username, octokit) ‚Üí QualitySignals
```
**Analyzes:**
- Fork ratio (>80% = "fork-heavy profile" flag)
- Tutorial repos in top 10
- Commit burst patterns
- PR review participation (score = reviewEvents.length * 5)
- Issue discussion participation (score = issueEvents.length * 3)
- Maintenance score (% of repos actively maintained)

**Output:** Overall quality score (0-100) with human-readable flags

**Verdict:** Comprehensive profile-level analysis. Good signal aggregation.

#### 1.5 Score Adjustment
```typescript
applyQualityAdjustment(baseScore, qualitySignals) ‚Üí { adjustedScore, adjustment, reason }
```
- **Adjustment range:** ¬±25% of base score
- **Quality delta:** `(qualityScore - 50) / 50` ‚Üí scales adjustment
- **Bounds:** Adjusted score capped at 0-99

**Example:**
- Base score: 80
- Quality: 25 (low) ‚Üí adjustment: -20 ‚Üí final: 60
- Quality: 95 (high) ‚Üí adjustment: +20 ‚Üí final: 100 ‚Üí capped at 99

**Verdict:** Fair adjustment mechanism, prevents score ceiling at 100.

---

## 2. Bias Detection ‚ùå MISSING

### 2.1 Current State: NONE

**NO mechanisms exist to:**
1. Detect protected class inference (age, gender, race, ethnicity, disability)
2. Check for demographic bias in scoring (e.g., location-based penalties)
3. Validate psychometric archetypes for bias (e.g., gender stereotyping)
4. Flag AI hallucinations or unsubstantiated claims
5. Warn about biased language in outreach messages
6. Monitor for disparate impact across candidate demographics

### 2.2 Bias Risk Points in Current System

#### Risk 1: Psychometric Archetype System (HIGH RISK)
**Location:** `services/ai/profiling.ts:36-73`

**Problem:** 12 fixed archetypes with gendered/stereotypical language:
- "The Strategic Scaler üöÄ" - favors rapid promotions (disadvantages women with career breaks)
- "The Hands-On Fixer üîß" - short tenure pattern (could disadvantage young parents)
- "The People Catalyst ü§ù" - "high-trust cultures" (could introduce gender bias in leadership assessment)
- "The Enterprise Navigator üè¢" - "politics-savvy" (could disadvantage neurodivergent candidates)

**Risk:** Archetypes may encode implicit bias. No validation against protected classes.

#### Risk 2: Location-Based Scoring
**Location:** `types.ts:126` (ScoreBreakdown includes `location` component)

**Problem:** Geographic bias without justification:
- Remote work makes location less relevant
- Could disadvantage candidates from underrepresented regions
- No transparency on how location is scored

**Risk:** Disparate impact on candidates from non-major tech hubs.

#### Risk 3: "Years Experience" Inference
**Location:** `services/ai/profiling.ts:79-87` (Career Trajectory analysis)

**Problem:** Infers seniority from tenure patterns:
- "Average tenure: Calculate typical time per role"
- "Tenure pattern: job-hopper (<2 years), stable (3+ years), long-term (5+ years)"

**Risk:** Penalizes career breaks (maternity/paternity leave, caregiving, illness) which disproportionately affect women and disabled candidates.

#### Risk 4: Compensation Intelligence
**Location:** `services/ai/profiling.ts:104-108`, `types.ts:165-170`

**Problem:** AI infers salary expectations:
```typescript
compensationIntelligence: {
  impliedSalaryBand: { min, max, currency },
  compensationGrowthRate: 'aggressive' | 'steady' | 'flat',
  likelySalaryExpectation: number
}
```

**Risk:** Gender pay gap amplification. If AI learns from historical data, it may perpetuate lower salary expectations for women.

#### Risk 5: "Red Flags" in Persona
**Location:** `types.ts:181`, `services/ai/profiling.ts:143`

**Problem:** AI generates "red flags" without bias checks:
- Could flag career breaks as "unexplained gaps"
- Could flag non-standard education paths
- Could flag non-Western communication styles

**Risk:** Discriminatory pattern matching.

### 2.3 What a Bias Validator Would Look Like

**Proposed:** `services/biasValidator.ts`

```typescript
interface BiasValidationResult {
  passed: boolean;
  warnings: BiasWarning[];
  blockers: BiasBlocker[];
  confidence: 'high' | 'medium' | 'low';
}

interface BiasWarning {
  category: 'protected_class' | 'stereotype' | 'hallucination' | 'disparate_impact';
  severity: 'critical' | 'high' | 'medium' | 'low';
  message: string;
  evidence: string;
  suggestion: string;
}
```

**Detection strategies:**

1. **Protected Class Inference Detection**
   - NLP scan for age/gender/race/disability references
   - Regex patterns: "young", "experienced professional", "native speaker", "cultural fit"
   - Flag: "This analysis may infer age from 'experienced professional'. Consider removing."

2. **Psychometric Archetype Bias Check**
   - Cross-reference archetype assignments with protected classes
   - Statistical analysis: Are women disproportionately assigned "People Catalyst"?
   - Detect gendered language: "he/she", "aggressive", "nurturing"

3. **Hallucination Detection**
   - Evidence tracing: Every claim must cite a source (GitHub event, LinkedIn data, resume)
   - Flag unsourced claims: "Candidate is detail-oriented" without evidence
   - Require confidence scores for inferences

4. **Disparate Impact Monitoring**
   - Track score distributions by demographic (requires opt-in demographic data)
   - Flag if location/tenure patterns correlate with protected classes
   - Generate audit reports: "Candidates from [region] score 15% lower on average"

5. **Language Bias Scanning**
   - Outreach message analysis: Scan for gendered pronouns, ageist language
   - Interview question validation: Flag questions that probe protected classes
   - "Culture fit" warnings: Flag vague assessments that could mask bias

**UI Integration:**
- Bias warning banner on profile pages
- Color-coded severity (red = blocker, yellow = warning, green = passed)
- Expandable details with suggestions for mitigation
- Opt-in bias audit log for compliance

---

## 3. EU AI Act Compliance ‚úÖ STRONG

### Location
- **Service:** `services/auditService.ts`
- **Migration:** `supabase/migrations/002_immutable_audit_logs.sql`
- **Docs:** `docs/EU_AI_ACT_COMPLIANCE.md`

### Capabilities

#### 3.1 Immutable Audit Logs
- **Append-only logging** to Supabase with SHA-256 hash chaining
- **localStorage fallback** for offline operation
- **Automatic sync queue** when reconnecting
- **Integrity verification** via `verify_audit_chain()` RPC

**Schema fields:**
```typescript
- id, event_type, created_at
- user_id, user_email
- description, credits_charged
- subject_id, subject_type (candidate/job/team/system)
- model_provider, model_version
- input_hash, output_hash (SHA-256, privacy-preserving)
- previous_hash, entry_hash (chain integrity)
- metadata (data_sources, evidence_count, confidence)
```

**Verdict:** Comprehensive audit trail. Meets Article 17 (record-keeping requirements).

#### 3.2 Comparative Analysis Mode (EU Compliant)
**Location:** `lib/services/gemini/comparativeAnalysis.ts`, `docs/EU_AI_ACT_COMPLIANCE.md`

**Change:** Replaced automated scoring (0-100) with factual comparisons:

**Before (High-Risk):**
```json
{ "alignmentScore": 85, "scoreBreakdown": { "skills": 28/35 } }
```

**After (Limited Risk):**
```json
{
  "skillsComparison": {
    "requiredSkills": [
      { "requirement": "React", "candidateEvidence": "15 repos", "match": "strong" }
    ]
  }
}
```

**Verdict:** ‚úÖ Compliant with Article 52 (transparency obligations). Removes automated decision-making.

#### 3.3 Compliance Gaps

**Gap 1: Transparency Notices**
- EU AI Act requires informing candidates they're being profiled
- **Missing:** Candidate-facing transparency notice
- **Needed:** Email/UI notification when candidate is analyzed

**Gap 2: Right to Explanation**
- Article 52 requires "meaningful information" about AI logic
- **Exists:** `buildCandidateExplanation()` in `lib/explainability.ts`
- **Missing:** Candidate-accessible explanation (currently only for recruiters)

**Gap 3: Right to Human Review**
- High-risk systems require human review of AI decisions
- **Exists:** Comparative mode ensures human-in-the-loop
- **Missing:** UI affordance for "Request Human Review" button

---

## 4. Fake Profile Detection ‚ùå WEAK

### 4.1 Current State: Indirect Only

**What exists:**
- Tutorial repo detection (flags learning projects)
- Commit burst detection (flags artificial activity)
- Fork ratio analysis (flags low-contribution profiles)

**What's missing:**
- **Synthetic GitHub profile detection** (e.g., GPT-generated READMEs)
- **Cloned profile detection** (duplicated content across accounts)
- **Sock puppet detection** (multiple accounts, same person)
- **AI-generated resume detection** (increasingly common)
- **LinkedIn authenticity validation** (scraped data may be fake)

### 4.2 Gartner Threat: 25% Fake Applications by 2028

**Context:** Gartner predicts 25% of job applications will contain fake or AI-generated content by 2028.

**RecruitOS vulnerability:**
- System trusts GitHub/LinkedIn data as authentic
- No verification that a GitHub account is actively used by the profile owner
- No cross-platform consistency checks (GitHub activity vs. LinkedIn history)
- No detection of AI-generated content (e.g., GPT-written project descriptions)

**Attack vectors:**
1. **Fake GitHub Repos:** Cloned popular repos with owner's name added to README
2. **Contribution Inflation:** Bots that auto-commit to repos to inflate activity graphs
3. **LinkedIn Impersonation:** Scraped profiles from real engineers, re-uploaded with fake contact info
4. **Resume Mills:** AI services that generate convincing fake work histories

### 4.3 What Fake Profile Detection Would Look Like

**Proposed:** `services/fakeProfileDetector.ts`

```typescript
interface FakeProfileRisk {
  overallRisk: 'low' | 'medium' | 'high' | 'critical';
  signals: FakeSignal[];
  confidence: number;
  recommendation: 'proceed' | 'verify' | 'reject';
}

interface FakeSignal {
  type: 'synthetic_content' | 'cloned_profile' | 'activity_anomaly' | 'cross_platform_mismatch';
  severity: 'critical' | 'high' | 'medium' | 'low';
  description: string;
  evidence: string;
}
```

**Detection strategies:**

1. **Synthetic Content Detection**
   - GPT detector on GitHub README, project descriptions
   - Flag overly generic or template-like bios
   - Check for consistency in writing style across repos

2. **Cloned Profile Detection**
   - Hash repos and check against known popular repos
   - Detect "forked-but-claimed-as-original" projects
   - Cross-reference commit authorship with account name

3. **Activity Anomaly Detection** (enhances existing anti-gaming)
   - Detect "too perfect" contribution graphs (bots often have uniform patterns)
   - Flag accounts with sudden activity spikes after long dormancy
   - Check for commit timestamp anomalies (all commits at 3 AM, etc.)

4. **Cross-Platform Consistency Checks**
   - Compare GitHub activity dates with LinkedIn employment dates
   - Flag mismatches (e.g., "Senior Engineer 2020-2023" but no GitHub activity in 2020)
   - Verify company exists and candidate worked there (via LinkedIn verification badges)

5. **Email/Identity Verification**
   - GitHub email vs. LinkedIn email consistency
   - Disposable email detection
   - Domain verification (work email from claimed employer)

**UI Integration:**
- Risk badge on candidate cards: üî¥ High Risk, üü° Verify, üü¢ Authentic
- Expandable risk report with evidence
- "Request Verification" workflow (e.g., video interview link, ID check)

---

## 5. Behavioral Signals Validation ‚ùå WEAK

### Location
- **Service:** `services/behavioralSignalsService.ts`
- **Tests:** `tests/services/behavioralSignalsService.test.ts`

### 5.1 What's Tracked (No Validation)

**GitHub Activity:**
- Contribution count, streak, top languages
- Recent repos, open source contributions
- Activity trend (increasing/stable/declining)

**Conference Speaking:**
- SERP search for "[name] speaker conference"
- Event name, date, topic extracted from search results

**Job Change Signals:**
- LinkedIn profile updates (title, company, location changes)
- "Open to Work" badge detection
- Recent profile updates (<7 days = "may be exploring options")

**Content Activity:**
- Medium, Dev.to, LinkedIn Pulse articles
- Platform, type, date, topic

### 5.2 Validation Gaps

**Problem 1: No authenticity checks**
- **GitHub activity** could be botted (auto-commits, forked contributions)
- **Conference speaking** uses SERP (not official conference sites) ‚Üí unreliable
- **LinkedIn signals** assume scraped data is accurate (no verification)
- **Content activity** doesn't verify authorship (could be ghostwritten)

**Problem 2: No cross-validation**
- Doesn't check if "speaker at ReactConf 2025" matches LinkedIn experience
- Doesn't verify if "active code reviewer" aligns with GitHub review count
- Doesn't flag inconsistencies (e.g., "Open to Work" but just joined new company)

**Problem 3: Naive inference**
- "Recent profile update = exploring options" is weak signal (could just be refreshing profile)
- "Activity declining = good time to approach" is speculative
- "Contribution streak > 30 days = reach out anytime" assumes availability

### 5.3 What Validation Would Look Like

**Proposed enhancements to `behavioralSignalsService.ts`:**

```typescript
interface ValidatedBehavioralSignals extends BehavioralSignals {
  validation: {
    githubAuthenticity: 'verified' | 'suspicious' | 'unknown';
    speakingEngagementsVerified: number; // Count of verified vs. SERP-sourced
    linkedinConsistency: 'consistent' | 'minor_gaps' | 'major_discrepancies';
    contentAuthorship: 'verified' | 'unverified';
  };
  warnings: string[];
}
```

**Validation strategies:**

1. **GitHub Authenticity**
   - Check commit email domains (personal vs. work)
   - Verify repo ownership vs. fork status
   - Flag "too perfect" contribution graphs (bot indicators)

2. **Speaking Engagements Verification**
   - Cross-reference with official conference sites (not just SERP)
   - Check YouTube/conference video platforms for actual talks
   - Verify speaker badge on LinkedIn

3. **LinkedIn Consistency**
   - Compare LinkedIn employment dates with GitHub activity timeline
   - Flag "Open to Work" with recent company change (<90 days)
   - Check if claimed skills match GitHub repo languages

4. **Content Authorship**
   - Verify Medium/Dev.to profiles link back to claimed GitHub/LinkedIn
   - Check article quality (long-form vs. low-effort listicles)
   - Flag if no engagement (0 claps, 0 comments = potential spam)

**UI Integration:**
- Trust score badge: ‚úÖ Verified, ‚ö†Ô∏è Unverified, üî¥ Suspicious
- Expandable validation report
- Manual verification workflow (e.g., "Contact candidate to verify speaking engagement")

---

## 6. Gap Analysis: What's Missing

### 6.1 Critical Gaps (Must-Have for v1.0)

| Gap | Impact | Effort | Priority |
|-----|--------|--------|----------|
| **Bias validator service** | Legal risk, recruiter trust | High (2-3 weeks) | P0 |
| **Fake profile detection** | Data integrity, wasted credits | Medium (1-2 weeks) | P0 |
| **Candidate transparency notices** | EU AI Act compliance | Low (3-5 days) | P0 |
| **Right to human review UI** | EU AI Act compliance | Low (2-3 days) | P1 |

### 6.2 High-Value Enhancements (v1.1+)

| Feature | Value Prop | Effort | Priority |
|---------|------------|--------|----------|
| **Disparate impact monitoring** | Trust & Safety dashboard for admins | High (2 weeks) | P1 |
| **Cross-platform consistency checks** | Higher data quality, fewer fake profiles | Medium (1 week) | P1 |
| **Psychometric archetype validation** | Scientific rigor, bias reduction | High (3-4 weeks) | P2 |
| **AI hallucination detection** | Trust, EU AI Act transparency | Medium (1-2 weeks) | P1 |

### 6.3 Nice-to-Have (Backlog)

- Real-time bias warnings in UI (inline, as recruiter types notes)
- Bias audit export for HR compliance teams
- Synthetic content detector for resumes
- Identity verification integration (e.g., ID.me, Onfido)

---

## 7. Proposed Feature: Bias Validator + Anti-Gaming Dashboard

### 7.1 Product Vision

**Goal:** Give recruiters a "trust score" for every candidate, combining anti-gaming quality signals with bias risk warnings.

**UI Concept:**

**Candidate Card:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üë§ John Doe                                 ‚îÇ
‚îÇ Senior Engineer @ Stripe                    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Quality Score: 85/100 ‚úÖ                    ‚îÇ
‚îÇ ‚îú‚îÄ Anti-Gaming: Low Risk                    ‚îÇ
‚îÇ ‚îú‚îÄ Profile Authenticity: Verified ‚úÖ        ‚îÇ
‚îÇ ‚îî‚îÄ Bias Check: 2 Warnings ‚ö†Ô∏è               ‚îÇ
‚îÇ    ‚îî‚îÄ Age inference detected                ‚îÇ
‚îÇ    ‚îî‚îÄ Career break flagged as gap           ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ [View Full Bias Report]  [Request Verify]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Bias Report Modal:**
```
‚ö†Ô∏è Bias Warnings (2)

1. Protected Class Inference: Age
   Severity: Medium
   Evidence: Persona archetype "experienced professional" may imply age.
   Suggestion: Remove age-related language before sharing with hiring team.

2. Career Gap Flagged
   Severity: Low
   Evidence: 18-month gap between roles (2020-2021).
   Context: This period overlaps with COVID-19. Consider asking candidate directly.
   Suggestion: Do not penalize without context.

‚úÖ No Critical Issues Found
```

### 7.2 Technical Architecture

**New files:**
- `services/biasValidator.ts` - Core bias detection logic
- `lib/bias-patterns.ts` - Protected class keyword library, regex patterns
- `components/BiasReportModal.tsx` - UI for bias warnings
- `components/QualityScoreBadge.tsx` - Combined quality + bias badge

**Integration points:**
- Call `biasValidator.validate()` in `/api/profile/analyze`
- Store bias validation results in Prisma `Candidate` model (new field: `biasValidation: Json?`)
- Surface warnings in pipeline UI, profile pages

**Data model:**
```typescript
interface BiasValidation {
  passed: boolean; // No critical issues
  warnings: BiasWarning[];
  validatedAt: string;
  validatorVersion: string; // Track validator updates
}
```

### 7.3 Rollout Strategy

**Phase 1: Logging + Monitoring (Week 1-2)**
- Ship bias validator in "observe-only" mode
- Log all warnings to audit logs
- No UI changes yet
- Collect baseline data on warning frequency

**Phase 2: Non-Blocking Warnings (Week 3-4)**
- Add warning badges to UI (yellow ‚ö†Ô∏è)
- Expandable bias report modal
- Warnings do NOT block candidate from pipeline
- Gather recruiter feedback

**Phase 3: Critical Blockers (Week 5+)**
- Introduce red üî¥ critical issues that recommend "Request Verification"
- Block AI-generated outreach if critical bias detected
- Add admin dashboard for disparate impact monitoring

---

## 8. Recommendations

### 8.1 Immediate Actions (This Sprint)

1. **Ship bias validator service** (observe-only mode)
   - Start logging protected class inferences
   - Identify top bias patterns in current data

2. **Add candidate transparency notice**
   - Email notification: "Your profile was analyzed by RecruitOS AI"
   - Link to explanation of how AI works (EU AI Act Article 52)

3. **Document current anti-gaming system**
   - Create admin-facing docs on quality score calculation
   - Publish confidence intervals for anti-gaming filters

### 8.2 Short-Term (Next 2 Sprints)

4. **Fake profile detection v1**
   - Cross-platform consistency checks (GitHub vs. LinkedIn)
   - Synthetic content detection on README/bio

5. **Bias warning UI**
   - Non-blocking warnings in candidate cards
   - Expandable bias report modal

6. **Right to human review**
   - Add "Request Human Review" button on candidate profiles
   - Route to internal review queue (manual recruiter review)

### 8.3 Long-Term (Roadmap)

7. **Disparate impact monitoring**
   - Admin dashboard showing score distributions by region/seniority
   - Quarterly bias audit reports for HR compliance

8. **Psychometric archetype validation**
   - Hire I/O psychologist consultant
   - Validate 12 archetypes against Big Five personality model
   - Remove or re-label biased archetypes

9. **Identity verification integration**
   - Partner with ID.me or Onfido for high-stakes roles
   - Optional "Verified Candidate" badge

---

## 9. Testing Strategy

### 9.1 Anti-Gaming Tests (Existing)
- ‚úÖ 17 tests in `tests/anti-gaming-filters.test.ts`
- ‚ö†Ô∏è 3 pre-existing failures (need triage)

**TODO:**
- Fix failing tests
- Add tests for edge cases (e.g., legitimate short-tenure profiles)
- Benchmark false positive rate (% of good candidates flagged as gaming)

### 9.2 Bias Validator Tests (New)
```typescript
// tests/services/biasValidator.test.ts
describe('BiasValidator', () => {
  it('detects age inference from "experienced professional"', () => {
    const result = validateBias({ archetype: 'Experienced professional with 20 years...' });
    expect(result.warnings).toContainEqual({ category: 'protected_class', evidence: 'age' });
  });

  it('flags gender-stereotyped archetype assignments', () => {
    const result = validateBias({ archetype: 'The People Catalyst ü§ù', gender: 'female' });
    expect(result.warnings).toContainEqual({ category: 'stereotype' });
  });

  it('allows neutral archetypes without warnings', () => {
    const result = validateBias({ archetype: 'The Domain Expert üìö' });
    expect(result.passed).toBe(true);
  });
});
```

### 9.3 E2E Tests
- **Scenario 1:** Low-quality profile ‚Üí quality score < 40, anti-gaming flag shown
- **Scenario 2:** High-quality profile with bias warning ‚Üí score 90, yellow ‚ö†Ô∏è badge
- **Scenario 3:** Fake profile detection ‚Üí red üî¥ "High Risk" badge, verify button

---

## 10. Open Questions

1. **Should bias validator block AI operations?**
   - Option A: Non-blocking (warnings only, recruiter decides)
   - Option B: Blocking (critical issues prevent AI-generated outreach)
   - **Recommendation:** Start non-blocking, escalate to blocking after data collection

2. **How to handle false positives in fake profile detection?**
   - Risk: Legitimate candidates flagged as fake lose opportunities
   - Mitigation: Manual review queue, "Request Verification" workflow
   - **Recommendation:** High false positive threshold (only flag clear fakes)

3. **Should we collect demographic data for disparate impact monitoring?**
   - Legal risk: GDPR, EU AI Act require explicit consent
   - Value: Can't detect bias without demographic labels
   - **Recommendation:** Opt-in demographic survey, anonymized aggregation

4. **What's the liability if AI generates biased assessments?**
   - EU AI Act: Providers liable for non-compliance
   - Insurance: Do we need AI liability coverage?
   - **Recommendation:** Consult legal team, consider insurance for high-risk customers

---

## Conclusion

**Strengths:**
- ‚úÖ Anti-gaming system is comprehensive and production-ready
- ‚úÖ EU AI Act compliance infrastructure is strong (audit logs, comparative analysis)

**Weaknesses:**
- ‚ùå NO bias detection ‚Üí legal risk, recruiter trust gap
- ‚ùå Fake profile detection is indirect only ‚Üí vulnerable to Gartner's 2028 threat
- ‚ùå Behavioral signals are unvalidated ‚Üí data quality concerns

**Priority:**
Ship **bias validator** (P0) and **fake profile detector** (P0) before scaling to enterprise customers. Recruiter trust depends on psychological safety: "I trust this AI won't get me sued."

**Next Steps:**
1. Review findings with team lead
2. Prioritize P0 features in feature plan (Task #6)
3. Spike bias validator implementation (1 week)

---

**Audit Complete.** Ready for synthesis phase.
